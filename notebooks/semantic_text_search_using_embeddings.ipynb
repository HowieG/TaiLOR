{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HowieG/TaiLOR/blob/vector-db/notebooks/semantic_text_search_using_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChZ0WvXdiE6f"
      },
      "source": [
        "## Semantic text search using embeddings\n",
        "\n",
        "We can search through all our reviews semantically in a very efficient manner and at very low cost, by embedding our search query, and then finding the most similar reviews. The dataset is created in the [Obtain_dataset Notebook](Obtain_dataset.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai transformers plotly tiktoken weaviate-client"
      ],
      "metadata": {
        "id": "uCZ1iQxukyTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import weaviate\n",
        "\n",
        "def create_client():\n",
        "    api_key = os.environ.get(\"TAILOR_WEAVIATE\")\n",
        "    openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "    client = weaviate.Client(\n",
        "        url = \"https://tailor-wiu5z0lk.weaviate.network\", \n",
        "        auth_client_secret=weaviate.AuthApiKey(api_key=api_key),\n",
        "        additional_headers = {\n",
        "            \"X-OpenAI-Api-Key\": openai_api_key  # Replace with your inference API key\n",
        "        }\n",
        "    )\n",
        "    return client"
      ],
      "metadata": {
        "id": "dnhxiI3FM0AC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = create_client()\n",
        "\n",
        "# ===== add schema =====\n",
        "product_obj = {\n",
        "    \"class\": \"Product\",\n",
        "    \"vectorizer\": \"text2vec-openai\"\n",
        "}\n",
        "\n",
        "client.schema.create_class(product_obj)\n",
        "\n",
        "image_obj = {\n",
        "    \"class\": \"Image\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "V7JDOe8-NCNZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://api.thenextleg.io/v2/describe\"\n",
        "\n",
        "payload = json.dumps({\n",
        "  \"url\": \"https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcQFpRS4kjadav4d65qRFEUa9m5DEN_fmphyU7wO_ssGjA7cWY0lKwPIfVdYN5bNS_hNUX5jV_V6dk2ZthW3ix5h04g-jD8ukbPZWH3sAsngi3gnZVMSMlnk3A&usqp=CAE\",\n",
        "  \"ref\": \"\",\n",
        "  \"webhookOverride\": \"\"\n",
        "})\n",
        "headers = {\n",
        "  'Authorization': 'Bearer ',\n",
        "  'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
        "\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "liC_dtqBnDyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "url = \"https://api.thenextleg.io/v2/message/XJhtBQAwsPo1kISrvDC3?expireMins=2\"\n",
        "\n",
        "headers = {\n",
        "  'Authorization': 'Bearer ',\n",
        "  'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "response = requests.request(\"GET\", url, headers=headers)\n",
        "\n",
        "print(response.text)\n",
        "                "
      ],
      "metadata": {
        "id": "rcJ9Ka4_JMqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install replicate"
      ],
      "metadata": {
        "id": "mWGo4BJ6TLgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"REPLICATE_API_TOKEN\"] = \"\"\n",
        "\n",
        "import replicate\n",
        "output = replicate.run(\n",
        "    \"methexis-inc/img2prompt:50adaf2d3ad20a6f911a8a9e3ccf777b263b8596fbd2c8fc26e8888f8a0edbb5\",\n",
        "    input={\"image\": open(\"2358824670_woman, skirt ends at knees _xl-beta-v2-2-2.png\", \"rb\")}\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "7dr13t7reFCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install midjourney-api"
      ],
      "metadata": {
        "id": "lOi02SMos0su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from midjourney_api import TNL\n",
        "\n",
        "TNL_API_KEY = ''\n",
        "tnl = TNL(TNL_API_KEY)\n",
        "image_url = \"https://encrypted-tbn0.gstatic.com/shopping?q=tbn:ANd9GcQFpRS4kjadav4d65qRFEUa9m5DEN_fmphyU7wO_ssGjA7cWY0lKwPIfVdYN5bNS_hNUX5jV_V6dk2ZthW3ix5h04g-jD8ukbPZWH3sAsngi3gnZVMSMlnk3A&usqp=CAE\"\n",
        "response = tnl.describe(image_url)\n",
        "\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "M5UGAZ7PtTad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_image(url, filename):\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img.save(filename)"
      ],
      "metadata": {
        "id": "UkR4qsvynM3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from serpapi import GoogleSearch\n",
        "import replicate\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import uuid\n",
        "\n",
        "\n",
        "def get_product_urls(query, num):\n",
        "    params = {\n",
        "        \"engine\": \"google\",\n",
        "        \"tbm\": \"shop\",\n",
        "        \"q\": query,\n",
        "        \"num\": num,\n",
        "        \"api_key\": os.environ.get(\"SERPAPI_KEY\")\n",
        "    }\n",
        "    \n",
        "    client = GoogleSearch(params)\n",
        "    results = client.get_dict()\n",
        "    \n",
        "    # Extract product and image URLs\n",
        "    for item in results:\n",
        "        if \"thumbnail\" in item:\n",
        "            download_image(item[\"thumbnail\"], 'image')\n",
        "            os.environ[\"REPLICATE_API_TOKEN\"] = \"\"\n",
        "\n",
        "            description = replicate.run(\n",
        "                \"methexis-inc/img2prompt:50adaf2d3ad20a6f911a8a9e3ccf777b263b8596fbd2c8fc26e8888f8a0edbb5\",\n",
        "                input={\"image\": open(\"image\", \"rb\")}\n",
        "            )\n",
        "\n",
        "            namespace = uuid.NAMESPACE_URL  # or any other predefined namespace\n",
        "\n",
        "            description_uuid = uuid.uuid5(namespace, item[\"link\"])\n",
        "            image_uuid = uuid.uuid5(namespace, item[\"thumbnail\"])\n",
        "\n",
        "\n",
        "            with client.batch as batch:\n",
        "                desc_properties = {\n",
        "                    \"description\": description,\n",
        "                    \"url\": item[\"link\"],\n",
        "                    \"image_url\": item[\"thumbnail\"]\n",
        "                }\n",
        "\n",
        "                client.batch.add_data_object(desc_properties, \"Product\", uuid=description_uuid)\n",
        "                \n",
        "                image_properties = {\n",
        "                    \"description_id\": description_uuid\n",
        "                }\n",
        "                client.batch.add_data_object(image_properties, \"Image\", vector=d[\"Vector\"], uuid=image_uuid)\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "sZ-PahKY7a5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55YxR7REN0ok"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "\n",
        "from openai.embeddings_utils import get_embedding\n",
        "import openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "DjIlgK_gMInP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "openai.api_key_path = \"key.txt\""
      ],
      "metadata": {
        "id": "LBSoRa5_W2bC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rszwIPfhN0ok"
      },
      "outputs": [],
      "source": [
        "# embedding model parameters\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "embedding_encoding = \"cl100k_base\"  # this the encoding for text-embedding-ada-002\n",
        "max_tokens = 8000  # the maximum for text-embedding-ada-002 is 8191\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7kUmqQkN0ok"
      },
      "outputs": [],
      "source": [
        "# Your list of descriptions\n",
        "image_descriptions = [\n",
        "    \"3 - Piece Upholstered Sectional couch, gray\",\n",
        "    \"a woman in a wedding dress posing for a picture, a digital rendering by LÃ¼ Ji, trending on pinterest, romanesque, rococo, white background, elegant\",\n",
        "    \"a black backpack with a thin blue line on it, a digital rendering by Jeff A. Menges, reddit contest winner, cobra, contest winner, sabattier filter, sabattier effect\",\n",
        "    \"a gray and green shirt hanging on a white wall, a stock photo by Jerry Wilkerson, pinterest contest winner, verdadism, y2k aesthetic, contest winner, clean\",\n",
        "    \"a woman in a wedding dress holding a bouquet of flowers, a digital rendering by Thomas Millie Dow, trending on cg society, arabesque, made of flowers, detailed, ornate\"\n",
        "]\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(image_descriptions, columns=['description'])\n",
        "\n",
        "# Print DataFrame\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEk6fcP3N0ol"
      },
      "outputs": [],
      "source": [
        "# subsample to 1k most recent reviews and remove samples that are too long\n",
        "top_n = 1000\n",
        "\n",
        "encoding = tiktoken.get_encoding(embedding_encoding)\n",
        "\n",
        "# omit reviews that are too long to embed\n",
        "df[\"n_tokens\"] = df.description.apply(lambda x: len(encoding.encode(x)))\n",
        "df = df[df.n_tokens <= max_tokens].tail(top_n)\n",
        "len(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1t-4S5nN0ol"
      },
      "source": [
        "## 2. Get embeddings and save them for future reuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8rrAnAXN0ol"
      },
      "outputs": [],
      "source": [
        "# Ensure you have your API key set in your environment per the README: https://github.com/openai/openai-python#usage\n",
        "\n",
        "# This may take a few minutes\n",
        "df[\"embedding\"] = df.description.apply(lambda x: get_embedding(x, engine=embedding_model))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AV7AHhfiE6g"
      },
      "source": [
        "Here we compare the cosine similarity of the embeddings of the query and the documents, and show top_n best matches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pnI11TPiE6g"
      },
      "outputs": [],
      "source": [
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "\n",
        "# search through the reviews for a specific product\n",
        "def search_reviews(df, product_description, n=3, pprint=True):\n",
        "    product_embedding = get_embedding(\n",
        "        product_description,\n",
        "        engine=\"text-embedding-ada-002\"\n",
        "    )\n",
        "    df[\"similarity\"] = df.embedding.apply(lambda x: cosine_similarity(x, product_embedding))\n",
        "\n",
        "    results = (\n",
        "        df.sort_values(\"similarity\", ascending=False)\n",
        "        .head(n)\n",
        "        .description\n",
        "    )\n",
        "    if pprint:\n",
        "        for r in results:\n",
        "            print(r[:200])\n",
        "            print()\n",
        "    return results\n",
        "\n",
        "\n",
        "results = search_reviews(df, \"a woman in a white dress holding a bouquet of flowers, a digital rendering by Irene and Laurette Patten, trending on pinterest, neoclassicism, elegant, white background, full body\", n=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZry5ScaiE6h"
      },
      "source": [
        "We can search through these reviews easily. To speed up computation, we can use a special algorithm, aimed at faster search through embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW5zp6OJiE6h"
      },
      "source": [
        "As we can see, this can immediately deliver a lot of value. In this example we show being able to quickly find the examples of delivery failures."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import CLIPProcessor, FlaxCLIPModel\n",
        "\n",
        "# set the model path\n",
        "model_path = \"openai/clip-vit-large-patch14\"\n",
        "# initialize the CLIPProcessor using the pretrained model\n",
        "processor = CLIPProcessor.from_pretrained(model_path)\n",
        "# initialize the FlaxCLIPModel using the pretrained model\n",
        "model = FlaxCLIPModel.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "BR1Nn2AQ9pN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8SOOL2vub65s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def list_files_in_folder(folder_path):\n",
        "    files_list = []\n",
        "    for subdir, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            files_list.append(os.path.join(subdir, file))\n",
        "    return files_list\n",
        "\n",
        "folder_path = '/content/drive/My Drive/tailor_images'\n",
        "files_list = list_files_in_folder(folder_path)\n",
        "image_paths = files_list[1:]\n",
        "target_image_path = files_list[0]"
      ],
      "metadata": {
        "id": "kSnKPz6bcDLO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from flax import linen as nn\n",
        "import optax\n",
        "\n",
        "def embed_images(images, target_image):\n",
        "    # Convert the target_image into model-acceptable input and apply padding\n",
        "    target_inputs = processor(images=target_image, return_tensors=\"jax\", padding=True)\n",
        "    target_emb = model.get_image_features(**target_inputs)\n",
        "    target_emb = jnp.array(target_emb.tolist())\n",
        "\n",
        "    # Normalize the target_emb\n",
        "    target_emb = target_emb / jnp.linalg.norm(target_emb)\n",
        "\n",
        "    # Convert the images into model-acceptable inputs and apply padding\n",
        "    inputs = processor(images=images, return_tensors=\"jax\", padding=True)\n",
        "    emb = model.get_image_features(**inputs)\n",
        "\n",
        "    # Convert the embedding vectors into a JAX array\n",
        "    emb = jnp.array(emb.tolist())\n",
        "\n",
        "    # Normalize the embeddings\n",
        "    emb = emb / jnp.expand_dims(jnp.linalg.norm(emb, axis=-1), axis=-1)\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    cos_similarities = jnp.dot(emb, target_emb.T)\n",
        "\n",
        "    # Convert the cosine similarities to a list and return\n",
        "    return cos_similarities.tolist()"
      ],
      "metadata": {
        "id": "WeAYfnw6-A-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def load_images(paths):\n",
        "    # create empty lists to store images and metadata\n",
        "    images = []\n",
        "    # loop through each file path in the input list\n",
        "    for path in paths:\n",
        "        # open the image at the file path and convert it to RGB format\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        # append the image to the list of images and include path in metadata\n",
        "        images.append(img)\n",
        "    # Return the lists of images and metadata\n",
        "    return images"
      ],
      "metadata": {
        "id": "1rvwyZnAUKL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def load_images(paths):\n",
        "    # create empty lists to store images and metadata\n",
        "    images = []\n",
        "    # loop through each file path in the input list\n",
        "    for path in paths:\n",
        "        # open the image at the file path and convert it to RGB format\n",
        "        img = Image.open(path).convert(\"RGB\")\n",
        "        # append the image to the list of images and include path in metadata\n",
        "        images.append(img)\n",
        "    # Return the lists of images\n",
        "    return images\n",
        "\n",
        "# Load your images using the function you defined\n",
        "target_image = load_images([target_image_path])[0]\n",
        "images = load_images(image_paths)\n",
        "\n",
        "# Now you can pass these images to the embed_images function\n",
        "cos_similarities = embed_images(images, target_image)\n",
        "print(cos_similarities)\n"
      ],
      "metadata": {
        "id": "n_43MovUzpQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files_list = [f for f in files_list if f != '/content/drive/My Drive/tailor_images/.DS_Store']\n",
        "images = load_images(files_list)\n",
        "embeddings = embed_images(images)"
      ],
      "metadata": {
        "id": "JE2z2G0TUcHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_images(images):\n",
        "  # converts the images into model-acceptable inputs and applies padding\n",
        "  inputs = processor(images=images, return_tensors=\"jax\", padding=True)\n",
        "  # passes the images through the CLIP model and extracts image features\n",
        "  emb = model.get_image_features(**inputs)\n",
        "  # converts the embedding vectors into a Python list and returns them\n",
        "  return emb.tolist()"
      ],
      "metadata": {
        "id": "kWcmhIJj7Xxk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "openai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "365536dcbde60510dc9073d6b991cd35db2d9bac356a11f5b64279a5e6708b97"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}